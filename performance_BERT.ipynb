{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim import utils\n",
    "from markdown import markdown\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "import gensim.parsing.preprocessing as gsp\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score, cross_val_p57838redict\n",
    "from sklearn.metrics import (\n",
    "    make_scorer,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    ")\n",
    "import random\n",
    "import torch\n",
    "import transformers as ppb\n",
    "import warnings\n",
    "import scipy.sparse\n",
    "from scipy.sparse import hstack\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "source": [
    "Randomly picks data from the files. Used for testing purposes as full data can't be run on machine. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.35\n",
    "r = 0.03\n",
    "q = 0.001\n",
    "s = 0.0004\n",
    "\n",
    "\n",
    "\n",
    "train_df = pd.read_csv(\"../data/filtered_typology_final.csv\", skiprows=lambda i: i>0 and random.random() > p)\n",
    "wow_unlabeled = pd.read_csv(\"../data/wow_uncoded_rules_codifying.csv\", skiprows=lambda i: i>0 and random.random() > r)\n",
    "minecraft_unlabeled = pd.read_csv(\"../data/minecraft_uncoded_rules_codifying.csv\", skiprows=lambda i: i>0 and random.random() > q)\n",
    "reddit_unlabeled = pd.read_csv(\"../data/reddit_uncoded_rules_codifying.csv\", skiprows=lambda i: i>0 and random.random() > s)\n",
    "\n",
    "unlabeled_list = [\"text\", \"communityID\", \"domain\"]\n",
    "wow_unlabeled, minecraft_unlabeled, reddit_unlabeled = (\n",
    "    wow_unlabeled[unlabeled_list],\n",
    "    minecraft_unlabeled[unlabeled_list],\n",
    "    reddit_unlabeled[unlabeled_list],\n",
    ")"
   ]
  },
  {
   "source": [
    "Picks out texts if the length of that text is less than 80, and places back into dataframe."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_df = train_df[train_df['text'].apply(lambda x: len(x) <= 80)]\n",
    "wow_unlabeled = wow_unlabeled[wow_unlabeled['text'].apply(lambda x: len(x) <= 80)]\n",
    "minecraft_unlabeled = minecraft_unlabeled[minecraft_unlabeled['text'].apply(lambda x: len(x) <= 80)]\n",
    "reddit_unlabeled = reddit_unlabeled[reddit_unlabeled['text'].apply(lambda x: len(x) <= 80)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of training data: \", train_df.shape)\n",
    "print(\"Shape of training data copy: \", train_one.shape)\n",
    "\n",
    "print(\"Shape of wow_unlabeled: \", wow_unlabeled.shape)\n",
    "print(\"Shape of minecraft_unlabeled: \", minecraft_unlabeled.shape)\n",
    "print(\"shape of reddit_unlabeled: \", reddit_unlabeled.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "- remove useless characters, whitespace, stopwords  \n",
    "- lowercasing \n",
    "- stemming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_html_markdown(s):\n",
    "    if type(s) not in [int, float] and s is not None:\n",
    "        return (\n",
    "            \" \".join(\n",
    "                re.split(\n",
    "                    \"[ _<>,.!|:#*\\n\\[\\]\\?]+\",\n",
    "                    \" \".join(\n",
    "                        BeautifulSoup(markdown(s), \"html.parser\").findAll(text=True)\n",
    "                    ),\n",
    "                )\n",
    "            )\n",
    "            .lower()\n",
    "            .strip()\n",
    "        )\n",
    "\n",
    "\n",
    "def whitespace_removal(df):\n",
    "    df.rule_norm_strategy = df.rule_norm_strategy.apply(lambda x: x.strip())\n",
    "    df.reg_const = df.reg_const.apply(lambda x: x.strip())\n",
    "    df.domain = df.domain.apply(lambda x: x.strip())\n",
    "    return df\n",
    "\n",
    "\n",
    "filters = [\n",
    "    gsp.strip_tags,\n",
    "    gsp.strip_punctuation,\n",
    "    gsp.strip_multiple_whitespaces,\n",
    "    gsp.strip_numeric,\n",
    "    gsp.remove_stopwords,\n",
    "    gsp.strip_short,\n",
    "    gsp.stem_text,\n",
    "]\n",
    "\n",
    "\n",
    "def clean_text(s):\n",
    "    if type(s) not in [int, float] and s is not None:\n",
    "        s = s.lower()\n",
    "        s = utils.to_unicode(s)\n",
    "        for f in filters:\n",
    "            s = f(s)\n",
    "        return s\n",
    "\n",
    "\n",
    "def randomShuffle(df):\n",
    "    return df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "def corpusGen(df):\n",
    "    return (\n",
    "        df.text.apply(strip_html_markdown)\n",
    "        .apply(lambda x: clean_text(x))\n",
    "        .astype(str)\n",
    "        .tolist()\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data transformation\n",
    "- generate corpus \n",
    "- transform the corpus to a normalized tf-idf representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpusGen(df):\n",
    "    return (\n",
    "        df.text.apply(strip_html_markdown)\n",
    "        .apply(lambda x: clean_text(x))\n",
    "        .astype(str)\n",
    "        .tolist()\n",
    "    )\n",
    "\n",
    "\n",
    "train_df = randomShuffle(whitespace_removal(train_df))\n",
    "IS_corpus = corpusGen(train_df)\n",
    "wow_corpus = corpusGen(wow_unlabeled)\n",
    "minecraft_corpus = corpusGen(minecraft_unlabeled)\n",
    "reddit_corpus = corpusGen(reddit_unlabeled)\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 3), stop_words=None)\n",
    "X_IS = vectorizer.fit_transform(IS_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_precision_recall_f1(l, category, accuracy):\n",
    "    precision_recall_df = pd.DataFrame(\n",
    "        l, columns=[\"Precision\", \"Recall\", \"F1 Score\", \"Support\"]\n",
    "    )\n",
    "    precision_recall_df.drop(\"Support\", axis=1, inplace=True)\n",
    "    precision_recall_df.insert(0, \"Type\", category)\n",
    "    precision_recall_df.insert(1, \"Accuracy\", accuracy)\n",
    "    return precision_recall_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "source": [
    "Helper functions for BERT implmentation:\n",
    "- padded_token_Generator : tokenizes and pads values to the same length\n",
    "- BERT_feature_Gen : Generates BERT features and returns features\n",
    "- Tfidf_BERT_Combine : Combines tfidf features with bert features, returns sparse matrix."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def returnDataframe(l):\n",
    "    temp = pd.DataFrame(l)\n",
    "    temp = temp.rename(columns =  {0:'text'})\n",
    "    return temp\n",
    "\n",
    "def padded_token_Generator(df):\n",
    "    token = df['text'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
    "    max_len = 0\n",
    "    for i in token.values:\n",
    "        if len(i) > max_len:\n",
    "            max_len = len(i)\n",
    "            padded = np.array([i + [0]*(max_len-len(i)) for i in token.values])\n",
    "    return padded \n",
    "\n",
    "def BERT_feature_Gen(padded_tokens):\n",
    "    attention_mask = np.where(padded_tokens != 0, 1, 0)\n",
    "    input_ids = torch.tensor(np.array(padded_tokens)).long()\n",
    "    attention_mask = torch.tensor(attention_mask)\n",
    "    with torch.no_grad():\n",
    "        last_hidden_states = model(input_ids)\n",
    "    features = last_hidden_states[0][:,0,:].numpy()\n",
    "    return features\n",
    "\n",
    "def Tfidf_BERT_Combine(tfidf_features,bert_features):\n",
    "    features_Matrix = scipy.sparse.csr_matrix(bert_features)\n",
    "    combined = hstack([tfidf_features,features_Matrix])\n",
    "    return combined\n",
    "\n"
   ]
  },
  {
   "source": [
    "# BERT Implementaion"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_X_IS = returnDataframe(IS_corpus)\n",
    "temp_wow_corpus = returnDataframe(wow_corpus)\n",
    "temp_minecraft_corpus = returnDataframe(minecraft_corpus)\n",
    "temp_reddit_corpus = returnDataframe(reddit_corpus)"
   ]
  },
  {
   "source": [
    "Tokenizing Data for BERT feature generation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = padded_token_Generator(temp_X_IS)\n",
    "tokenized_wow = padded_token_Generator(temp_wow_corpus)\n",
    "tokenized_mine = padded_token_Generator(temp_minecraft_corpus)\n",
    "tokenized_reddit = padded_token_Generator(temp_reddit_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_IS_BERT = BERT_feature_Gen(tokenized)\n",
    "X_IS_Combined = Tfidf_BERT_Combine(X_IS,X_IS_BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"BERT features shape: \", X_IS_BERT.shape)\n",
    "print(\"TF-IDF features shape: \", X_IS.shape)\n",
    "print(\"Combined features shape: \", X_IS_Combined.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_IS = []\n",
    "acc_IS = []\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "y_IS = train_df.IS.values.tolist()\n",
    "clf_IS = OneVsRestClassifier(LinearSVC())\n",
    "accuracy = cross_val_score(clf_IS, X_IS_Combined, y_IS, cv=kfold).mean()\n",
    "clf_IS.fit(X_IS_Combined, y_IS)\n",
    "y_IS_pred = cross_val_predict(clf_IS, X_IS_Combined, y_IS, cv=kfold)\n",
    "acc_IS.append(accuracy)\n",
    "l_IS.append(precision_recall_fscore_support(y_IS, y_IS_pred, average=\"weighted\"))\n",
    "IS_score = get_precision_recall_f1(l_IS, \"IS\", acc_IS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wow_BERT = BERT_feature_Gen(tokenized_wow)\n",
    "mine_BERT = BERT_feature_Gen(tokenized_mine)\n",
    "reddit_BERT = BERT_feature_Gen(tokenized_reddit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_X_wow, IS_X_minecraft, IS_X_reddit = (\n",
    "    vectorizer.transform(wow_corpus),\n",
    "    vectorizer.transform(minecraft_corpus),\n",
    "    vectorizer.transform(reddit_corpus),\n",
    ")\n",
    "\n",
    "WOW_X_IS = Tfidf_BERT_Combine(IS_X_wow,wow_BERT)\n",
    "Minecraft_X_IS = Tfidf_BERT_Combine(IS_X_minecraft,mine_BERT)\n",
    "Reddit_IS = Tfidf_BERT_Combine(IS_X_reddit,reddit_BERT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"BERT features shape: \", wow_BERT.shape)\n",
    "print(\"TF-IDF features shape: \", IS_X_wow.shape)\n",
    "print(\"Comined features shape: \", WOW_X_IS.shape)\n",
    "\n",
    "print(\"BERT features shape: \", mine_BERT.shape)\n",
    "print(\"TF-IDF features shape: \", IS_X_minecraft.shape)\n",
    "print(\"Comined features shape: \", Minecraft_X_IS.shape)\n",
    "\n",
    "print(\"BERT features shape: \", reddit_BERT.shape)\n",
    "print(\"TF-IDF features shape: \", IS_X_reddit.shape)\n",
    "print(\"Comined features shape: \", Reddit_IS.shape)\n"
   ]
  },
  {
   "source": [
    "Predicting on wow, minecraft, and reddit data using BERT and TFIDF Features"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wow_unlabeled[\"IS\"] = clf_IS.predict(WOW_X_IS)\n",
    "minecraft_unlabeled[\"IS\"] = clf_IS.predict(Minecraft_X_IS)\n",
    "reddit_unlabeled[\"IS\"] = clf_IS.predict(Reddit_IS)\n",
    "\n",
    "coded_df = train_df[unlabeled_list].copy()\n",
    "coded_df[\"IS\"] = clf_IS.predict(X_IS_Combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df[\n",
    "    (train_df[\"IS\"] == 1)\n",
    "    & (train_df[\"domain\"] == \"reddit\")\n",
    "    & (train_df[\"reg_const\"] != \"none\")\n",
    "]\n",
    "\n",
    "wow_unlabeled = wow_unlabeled[wow_unlabeled[\"IS\"] == 1]\n",
    "minecraft_unlabeled = minecraft_unlabeled[minecraft_unlabeled[\"IS\"] == 1]\n",
    "reddit_unlabeled = reddit_unlabeled[reddit_unlabeled[\"IS\"] == 1]\n",
    "coded_df = coded_df[coded_df[\"IS\"] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_wow_corpus = corpusGen(wow_unlabeled)\n",
    "rules_minecraft_corpus = corpusGen(minecraft_unlabeled)\n",
    "rules_reddit_corpus = corpusGen(reddit_unlabeled)\n",
    "\n",
    "rules_corpus = corpusGen(train_df)\n",
    "rules_coded_corpus = corpusGen(coded_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_rules = vectorizer.fit_transform(rules_corpus)\n",
    "X_coded = vectorizer.transform(rules_coded_corpus)\n",
    "X_wow = vectorizer.transform(rules_wow_corpus)\n",
    "X_minecraft = vectorizer.transform(rules_minecraft_corpus)\n",
    "X_reddit = vectorizer.transform(rules_reddit_corpus)\n"
   ]
  },
  {
   "source": [
    "Tokenizing rules to prep for BERT Feature generation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_X_rules = returnDataframe(rules_corpus)\n",
    "temp_X_coded = returnDataframe(rules_coded_corpus)\n",
    "temp_X_wow = returnDataframe(rules_wow_corpus)\n",
    "temp_X_minecraft = returnDataframe(rules_minecraft_corpus)\n",
    "temp_X_reddit = returnDataframe(rules_reddit_corpus)\n",
    "\n",
    "\n",
    "tokenized_X_rules = padded_token_Generator(temp_X_rules)\n",
    "tokenized_X_coded = padded_token_Generator(temp_X_coded)\n",
    "tokenized_X_wow = padded_token_Generator(temp_X_wow)\n",
    "tokenized_X_minecraft = padded_token_Generator(temp_X_minecraft)\n",
    "tokenized_X_reddit = padded_token_Generator(temp_X_reddit)\n"
   ]
  },
  {
   "source": [
    "Generating features for BERT and combining with TF-IDF"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_rules_BERT = BERT_feature_Gen(tokenized_X_rules)\n",
    "X_coded_BERT = BERT_feature_Gen(tokenized_X_coded)\n",
    "X_wow_BERT = BERT_feature_Gen(tokenized_X_wow)\n",
    "X_minecraft_BERT = BERT_feature_Gen(tokenized_X_minecraft)\n",
    "X_reddit_BERT = BERT_feature_Gen(tokenized_X_reddit)\n",
    "\n",
    "\n",
    "X_rules_Combined = Tfidf_BERT_Combine(X_rules,X_rules_BERT)\n",
    "X_coded_Combined = Tfidf_BERT_Combine(X_coded,X_coded_BERT)\n",
    "X_wow_Combined = Tfidf_BERT_Combine(X_wow,X_wow_BERT)\n",
    "X_minecraft_Combined = Tfidf_BERT_Combine(X_minecraft,X_minecraft_BERT)\n",
    "X_reddit_Combined = Tfidf_BERT_Combine(X_reddit,X_reddit_BERT)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_rules = []\n",
    "acc_rules = []\n",
    "categories = [\n",
    "    \"reg_const\",\n",
    "    \"rule_norm_strategy\",\n",
    "    \"position_type\",\n",
    "    \"boundary_type\",\n",
    "    \"aggregation_type\",\n",
    "    \"payoff_type\",\n",
    "    \"information_type\",\n",
    "    \"communication_type\",\n",
    "    \"choice_type\",\n",
    "    \"scope_type\",\n",
    "]\n",
    "\n",
    "clf_rules = OneVsRestClassifier(LinearSVC())\n",
    "\n",
    "for c in categories:\n",
    "    y_rules = train_df[c].values.tolist()\n",
    "    accuracy_rules = cross_val_score(clf_rules, X_rules_Combined, y_rules, cv=kfold).mean()\n",
    "    y_rules_pred = cross_val_predict(clf_rules, X_rules_Combined, y_rules, cv=kfold)\n",
    "    acc_rules.append(accuracy_rules)\n",
    "    l_rules.append(\n",
    "        precision_recall_fscore_support(y_rules, y_rules_pred, average=\"weighted\")\n",
    "    )\n",
    "    clf_rules.fit(X_rules_Combined, y_rules)\n",
    "\n",
    "    wow_unlabeled[c] = clf_rules.predict(X_wow_Combined)\n",
    "    minecraft_unlabeled[c] = clf_rules.predict(X_minecraft_Combined)\n",
    "    reddit_unlabeled[c] = clf_rules.predict(X_reddit_Combined)\n",
    "    coded_df[c] = clf_rules.predict(X_coded_Combined)\n",
    "\n",
    "rules_scores = get_precision_recall_f1(l_rules, categories, acc_rules)\n",
    "pd.concat([IS_score, rules_scores])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wow_unlabeled.to_csv(\"../output/wow_labeled.csv\", index=False)\n",
    "minecraft_unlabeled.to_csv(\"../output/minecraft_labeled.csv\", index=False)\n",
    "reddit_unlabeled.to_csv(\"../output/reddit_labeled.csv\", index=False)\n",
    "coded_df.to_csv(\"../output/coded_labeled.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}